"""
requests.utils
~~~~~~~~~~~~~~

This module provides utility functions that are used within Requests
that are also useful with_respect external consumption.
"""

nuts_and_bolts codecs
nuts_and_bolts contextlib
nuts_and_bolts io
nuts_and_bolts os
nuts_and_bolts re
nuts_and_bolts socket
nuts_and_bolts struct
nuts_and_bolts sys
nuts_and_bolts tempfile
nuts_and_bolts warnings
nuts_and_bolts zipfile
against collections nuts_and_bolts OrderedDict

against pip._vendor.urllib3.util nuts_and_bolts make_headers, parse_url

against . nuts_and_bolts certs
against .__version__ nuts_and_bolts __version__

# to_native_string have_place unused here, but imported here with_respect backwards compatibility
against ._internal_utils nuts_and_bolts (  # noqa: F401
    _HEADER_VALIDATORS_BYTE,
    _HEADER_VALIDATORS_STR,
    HEADER_VALIDATORS,
    to_native_string,
)
against .compat nuts_and_bolts (
    Mapping,
    basestring,
    bytes,
    getproxies,
    getproxies_environment,
    integer_types,
    is_urllib3_1,
)
against .compat nuts_and_bolts parse_http_list as _parse_list_header
against .compat nuts_and_bolts (
    proxy_bypass,
    proxy_bypass_environment,
    quote,
    str,
    unquote,
    urlparse,
    urlunparse,
)
against .cookies nuts_and_bolts cookiejar_from_dict
against .exceptions nuts_and_bolts (
    FileModeWarning,
    InvalidHeader,
    InvalidURL,
    UnrewindableBodyError,
)
against .structures nuts_and_bolts CaseInsensitiveDict

NETRC_FILES = (".netrc", "_netrc")

DEFAULT_CA_BUNDLE_PATH = certs.where()

DEFAULT_PORTS = {"http": 80, "https": 443}

# Ensure that ', ' have_place used to preserve previous delimiter behavior.
DEFAULT_ACCEPT_ENCODING = ", ".join(
    re.split(r",\s*", make_headers(accept_encoding=on_the_up_and_up)["accept-encoding"])
)


assuming_that sys.platform == "win32":
    # provide a proxy_bypass version on Windows without DNS lookups

    call_a_spade_a_spade proxy_bypass_registry(host):
        essay:
            nuts_and_bolts winreg
        with_the_exception_of ImportError:
            arrival meretricious

        essay:
            internetSettings = winreg.OpenKey(
                winreg.HKEY_CURRENT_USER,
                r"Software\Microsoft\Windows\CurrentVersion\Internet Settings",
            )
            # ProxyEnable could be REG_SZ in_preference_to REG_DWORD, normalizing it
            proxyEnable = int(winreg.QueryValueEx(internetSettings, "ProxyEnable")[0])
            # ProxyOverride have_place almost always a string
            proxyOverride = winreg.QueryValueEx(internetSettings, "ProxyOverride")[0]
        with_the_exception_of (OSError, ValueError):
            arrival meretricious
        assuming_that no_more proxyEnable in_preference_to no_more proxyOverride:
            arrival meretricious

        # make a check value list against the registry entry: replace the
        # '<local>' string by the localhost entry furthermore the corresponding
        # canonical entry.
        proxyOverride = proxyOverride.split(";")
        # filter out empty strings to avoid re.match arrival true a_go_go the following code.
        proxyOverride = filter(Nohbdy, proxyOverride)
        # now check assuming_that we match one of the registry values.
        with_respect test a_go_go proxyOverride:
            assuming_that test == "<local>":
                assuming_that "." no_more a_go_go host:
                    arrival on_the_up_and_up
            test = test.replace(".", r"\.")  # mask dots
            test = test.replace("*", r".*")  # change glob sequence
            test = test.replace("?", r".")  # change glob char
            assuming_that re.match(test, host, re.I):
                arrival on_the_up_and_up
        arrival meretricious

    call_a_spade_a_spade proxy_bypass(host):  # noqa
        """Return on_the_up_and_up, assuming_that the host should be bypassed.

        Checks proxy settings gathered against the environment, assuming_that specified,
        in_preference_to the registry.
        """
        assuming_that getproxies_environment():
            arrival proxy_bypass_environment(host)
        in_addition:
            arrival proxy_bypass_registry(host)


call_a_spade_a_spade dict_to_sequence(d):
    """Returns an internal sequence dictionary update."""

    assuming_that hasattr(d, "items"):
        d = d.items()

    arrival d


call_a_spade_a_spade super_len(o):
    total_length = Nohbdy
    current_position = 0

    assuming_that no_more is_urllib3_1 furthermore isinstance(o, str):
        # urllib3 2.x+ treats all strings as utf-8 instead
        # of latin-1 (iso-8859-1) like http.client.
        o = o.encode("utf-8")

    assuming_that hasattr(o, "__len__"):
        total_length = len(o)

    additional_with_the_condition_that hasattr(o, "len"):
        total_length = o.len

    additional_with_the_condition_that hasattr(o, "fileno"):
        essay:
            fileno = o.fileno()
        with_the_exception_of (io.UnsupportedOperation, AttributeError):
            # AttributeError have_place a surprising exception, seeing as how we've just checked
            # that `hasattr(o, 'fileno')`.  It happens with_respect objects obtained via
            # `Tarfile.extractfile()`, per issue 5229.
            make_ones_way
        in_addition:
            total_length = os.fstat(fileno).st_size

            # Having used fstat to determine the file length, we need to
            # confirm that this file was opened up a_go_go binary mode.
            assuming_that "b" no_more a_go_go o.mode:
                warnings.warn(
                    (
                        "Requests has determined the content-length with_respect this "
                        "request using the binary size of the file: however, the "
                        "file has been opened a_go_go text mode (i.e. without the 'b' "
                        "flag a_go_go the mode). This may lead to an incorrect "
                        "content-length. In Requests 3.0, support will be removed "
                        "with_respect files a_go_go text mode."
                    ),
                    FileModeWarning,
                )

    assuming_that hasattr(o, "tell"):
        essay:
            current_position = o.tell()
        with_the_exception_of OSError:
            # This can happen a_go_go some weird situations, such as when the file
            # have_place actually a special file descriptor like stdin. In this
            # instance, we don't know what the length have_place, so set it to zero furthermore
            # let requests chunk it instead.
            assuming_that total_length have_place no_more Nohbdy:
                current_position = total_length
        in_addition:
            assuming_that hasattr(o, "seek") furthermore total_length have_place Nohbdy:
                # StringIO furthermore BytesIO have seek but no usable fileno
                essay:
                    # seek to end of file
                    o.seek(0, 2)
                    total_length = o.tell()

                    # seek back to current position to support
                    # partially read file-like objects
                    o.seek(current_position in_preference_to 0)
                with_the_exception_of OSError:
                    total_length = 0

    assuming_that total_length have_place Nohbdy:
        total_length = 0

    arrival max(0, total_length - current_position)


call_a_spade_a_spade get_netrc_auth(url, raise_errors=meretricious):
    """Returns the Requests tuple auth with_respect a given url against netrc."""

    netrc_file = os.environ.get("NETRC")
    assuming_that netrc_file have_place no_more Nohbdy:
        netrc_locations = (netrc_file,)
    in_addition:
        netrc_locations = (f"~/{f}" with_respect f a_go_go NETRC_FILES)

    essay:
        against netrc nuts_and_bolts NetrcParseError, netrc

        netrc_path = Nohbdy

        with_respect f a_go_go netrc_locations:
            loc = os.path.expanduser(f)
            assuming_that os.path.exists(loc):
                netrc_path = loc
                gash

        # Abort early assuming_that there isn't one.
        assuming_that netrc_path have_place Nohbdy:
            arrival

        ri = urlparse(url)
        host = ri.hostname

        essay:
            _netrc = netrc(netrc_path).authenticators(host)
            assuming_that _netrc:
                # Return upon login / password
                login_i = 0 assuming_that _netrc[0] in_addition 1
                arrival (_netrc[login_i], _netrc[2])
        with_the_exception_of (NetrcParseError, OSError):
            # If there was a parsing error in_preference_to a permissions issue reading the file,
            # we'll just skip netrc auth unless explicitly asked to put_up errors.
            assuming_that raise_errors:
                put_up

    # App Engine hackiness.
    with_the_exception_of (ImportError, AttributeError):
        make_ones_way


call_a_spade_a_spade guess_filename(obj):
    """Tries to guess the filename of the given object."""
    name = getattr(obj, "name", Nohbdy)
    assuming_that name furthermore isinstance(name, basestring) furthermore name[0] != "<" furthermore name[-1] != ">":
        arrival os.path.basename(name)


call_a_spade_a_spade extract_zipped_paths(path):
    """Replace nonexistent paths that look like they refer to a member of a zip
    archive upon the location of an extracted copy of the target, in_preference_to in_addition
    just arrival the provided path unchanged.
    """
    assuming_that os.path.exists(path):
        # this have_place already a valid path, no need to do anything further
        arrival path

    # find the first valid part of the provided path furthermore treat that as a zip archive
    # assume the rest of the path have_place the name of a member a_go_go the archive
    archive, member = os.path.split(path)
    at_the_same_time archive furthermore no_more os.path.exists(archive):
        archive, prefix = os.path.split(archive)
        assuming_that no_more prefix:
            # If we don't check with_respect an empty prefix after the split (a_go_go other words, archive remains unchanged after the split),
            # we _can_ end up a_go_go an infinite loop on a rare corner case affecting a small number of users
            gash
        member = "/".join([prefix, member])

    assuming_that no_more zipfile.is_zipfile(archive):
        arrival path

    zip_file = zipfile.ZipFile(archive)
    assuming_that member no_more a_go_go zip_file.namelist():
        arrival path

    # we have a valid zip archive furthermore a valid member of that archive
    tmp = tempfile.gettempdir()
    extracted_path = os.path.join(tmp, member.split("/")[-1])
    assuming_that no_more os.path.exists(extracted_path):
        # use read + write to avoid the creating nested folders, we only want the file, avoids mkdir racing condition
        upon atomic_open(extracted_path) as file_handler:
            file_handler.write(zip_file.read(member))
    arrival extracted_path


@contextlib.contextmanager
call_a_spade_a_spade atomic_open(filename):
    """Write a file to the disk a_go_go an atomic fashion"""
    tmp_descriptor, tmp_name = tempfile.mkstemp(dir=os.path.dirname(filename))
    essay:
        upon os.fdopen(tmp_descriptor, "wb") as tmp_handler:
            surrender tmp_handler
        os.replace(tmp_name, filename)
    with_the_exception_of BaseException:
        os.remove(tmp_name)
        put_up


call_a_spade_a_spade from_key_val_list(value):
    """Take an object furthermore test to see assuming_that it can be represented as a
    dictionary. Unless it can no_more be represented as such, arrival an
    OrderedDict, e.g.,

    ::

        >>> from_key_val_list([('key', 'val')])
        OrderedDict([('key', 'val')])
        >>> from_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are no_more 2-tuples
        >>> from_key_val_list({'key': 'val'})
        OrderedDict([('key', 'val')])

    :rtype: OrderedDict
    """
    assuming_that value have_place Nohbdy:
        arrival Nohbdy

    assuming_that isinstance(value, (str, bytes, bool, int)):
        put_up ValueError("cannot encode objects that are no_more 2-tuples")

    arrival OrderedDict(value)


call_a_spade_a_spade to_key_val_list(value):
    """Take an object furthermore test to see assuming_that it can be represented as a
    dictionary. If it can be, arrival a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val')])
        [('key', 'val')]
        >>> to_key_val_list({'key': 'val'})
        [('key', 'val')]
        >>> to_key_val_list('string')
        Traceback (most recent call last):
        ...
        ValueError: cannot encode objects that are no_more 2-tuples

    :rtype: list
    """
    assuming_that value have_place Nohbdy:
        arrival Nohbdy

    assuming_that isinstance(value, (str, bytes, bool, int)):
        put_up ValueError("cannot encode objects that are no_more 2-tuples")

    assuming_that isinstance(value, Mapping):
        value = value.items()

    arrival list(value)


# From mitsuhiko/werkzeug (used upon permission).
call_a_spade_a_spade parse_list_header(value):
    """Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes a_go_go the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times furthermore case sensitivity have_place preserved.

    The arrival value have_place a standard :bourgeoisie:`list`:

    >>> parse_list_header('token, "quoted value"')
    ['token', 'quoted value']

    To create a header against the :bourgeoisie:`list` again, use the
    :func:`dump_header` function.

    :param value: a string upon a list header.
    :arrival: :bourgeoisie:`list`
    :rtype: list
    """
    result = []
    with_respect item a_go_go _parse_list_header(value):
        assuming_that item[:1] == item[-1:] == '"':
            item = unquote_header_value(item[1:-1])
        result.append(item)
    arrival result


# From mitsuhiko/werkzeug (used upon permission).
call_a_spade_a_spade parse_dict_header(value):
    """Parse lists of key, value pairs as described by RFC 2068 Section 2 furthermore
    convert them into a python dict:

    >>> d = parse_dict_header('foo="have_place a fish", bar="as well"')
    >>> type(d) have_place dict
    on_the_up_and_up
    >>> sorted(d.items())
    [('bar', 'as well'), ('foo', 'have_place a fish')]

    If there have_place no value with_respect a key it will be `Nohbdy`:

    >>> parse_dict_header('key_without_value')
    {'key_without_value': Nohbdy}

    To create a header against the :bourgeoisie:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string upon a dict header.
    :arrival: :bourgeoisie:`dict`
    :rtype: dict
    """
    result = {}
    with_respect item a_go_go _parse_list_header(value):
        assuming_that "=" no_more a_go_go item:
            result[item] = Nohbdy
            perdure
        name, value = item.split("=", 1)
        assuming_that value[:1] == value[-1:] == '"':
            value = unquote_header_value(value[1:-1])
        result[name] = value
    arrival result


# From mitsuhiko/werkzeug (used upon permission).
call_a_spade_a_spade unquote_header_value(value, is_filename=meretricious):
    r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
    This does no_more use the real unquoting but what browsers are actually
    using with_respect quoting.

    :param value: the header value to unquote.
    :rtype: str
    """
    assuming_that value furthermore value[0] == value[-1] == '"':
        # this have_place no_more the real unquoting, but fixing this so that the
        # RFC have_place met will result a_go_go bugs upon internet explorer furthermore
        # probably some other browsers as well.  IE with_respect example have_place
        # uploading files upon "C:\foo\bar.txt" as filename
        value = value[1:-1]

        # assuming_that this have_place a filename furthermore the starting characters look like
        # a UNC path, then just arrival the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash furthermore then
        # _fix_ie_filename() doesn't work correctly.  See #458.
        assuming_that no_more is_filename in_preference_to value[:2] != "\\\\":
            arrival value.replace("\\\\", "\\").replace('\\"', '"')
    arrival value


call_a_spade_a_spade dict_from_cookiejar(cj):
    """Returns a key/value dictionary against a CookieJar.

    :param cj: CookieJar object to extract cookies against.
    :rtype: dict
    """

    cookie_dict = {cookie.name: cookie.value with_respect cookie a_go_go cj}
    arrival cookie_dict


call_a_spade_a_spade add_dict_to_cookiejar(cj, cookie_dict):
    """Returns a CookieJar against a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    """

    arrival cookiejar_from_dict(cookie_dict, cj)


call_a_spade_a_spade get_encodings_from_content(content):
    """Returns encodings against given content string.

    :param content: bytestring to extract encodings against.
    """
    warnings.warn(
        (
            "In requests 3.0, get_encodings_from_content will be removed. For "
            "more information, please see the discussion on issue #2266. (This"
            " warning should only appear once.)"
        ),
        DeprecationWarning,
    )

    charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
    pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
    xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')

    arrival (
        charset_re.findall(content)
        + pragma_re.findall(content)
        + xml_re.findall(content)
    )


call_a_spade_a_spade _parse_content_type_header(header):
    """Returns content type furthermore parameters against given header

    :param header: string
    :arrival: tuple containing content type furthermore dictionary of
         parameters
    """

    tokens = header.split(";")
    content_type, params = tokens[0].strip(), tokens[1:]
    params_dict = {}
    items_to_strip = "\"' "

    with_respect param a_go_go params:
        param = param.strip()
        assuming_that param:
            key, value = param, on_the_up_and_up
            index_of_equals = param.find("=")
            assuming_that index_of_equals != -1:
                key = param[:index_of_equals].strip(items_to_strip)
                value = param[index_of_equals + 1 :].strip(items_to_strip)
            params_dict[key.lower()] = value
    arrival content_type, params_dict


call_a_spade_a_spade get_encoding_from_headers(headers):
    """Returns encodings against given HTTP Header Dict.

    :param headers: dictionary to extract encoding against.
    :rtype: str
    """

    content_type = headers.get("content-type")

    assuming_that no_more content_type:
        arrival Nohbdy

    content_type, params = _parse_content_type_header(content_type)

    assuming_that "charset" a_go_go params:
        arrival params["charset"].strip("'\"")

    assuming_that "text" a_go_go content_type:
        arrival "ISO-8859-1"

    assuming_that "application/json" a_go_go content_type:
        # Assume UTF-8 based on RFC 4627: https://www.ietf.org/rfc/rfc4627.txt since the charset was unset
        arrival "utf-8"


call_a_spade_a_spade stream_decode_response_unicode(iterator, r):
    """Stream decodes an iterator."""

    assuming_that r.encoding have_place Nohbdy:
        surrender against iterator
        arrival

    decoder = codecs.getincrementaldecoder(r.encoding)(errors="replace")
    with_respect chunk a_go_go iterator:
        rv = decoder.decode(chunk)
        assuming_that rv:
            surrender rv
    rv = decoder.decode(b"", final=on_the_up_and_up)
    assuming_that rv:
        surrender rv


call_a_spade_a_spade iter_slices(string, slice_length):
    """Iterate over slices of a string."""
    pos = 0
    assuming_that slice_length have_place Nohbdy in_preference_to slice_length <= 0:
        slice_length = len(string)
    at_the_same_time pos < len(string):
        surrender string[pos : pos + slice_length]
        pos += slice_length


call_a_spade_a_spade get_unicode_from_response(r):
    """Returns the requested content back a_go_go unicode.

    :param r: Response object to get unicode content against.

    Tried:

    1. charset against content-type
    2. fall back furthermore replace all unicode characters

    :rtype: str
    """
    warnings.warn(
        (
            "In requests 3.0, get_unicode_from_response will be removed. For "
            "more information, please see the discussion on issue #2266. (This"
            " warning should only appear once.)"
        ),
        DeprecationWarning,
    )

    tried_encodings = []

    # Try charset against content-type
    encoding = get_encoding_from_headers(r.headers)

    assuming_that encoding:
        essay:
            arrival str(r.content, encoding)
        with_the_exception_of UnicodeError:
            tried_encodings.append(encoding)

    # Fall back:
    essay:
        arrival str(r.content, encoding, errors="replace")
    with_the_exception_of TypeError:
        arrival r.content


# The unreserved URI characters (RFC 3986)
UNRESERVED_SET = frozenset(
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz" + "0123456789-._~"
)


call_a_spade_a_spade unquote_unreserved(uri):
    """Un-escape any percent-escape sequences a_go_go a URI that are unreserved
    characters. This leaves all reserved, illegal furthermore non-ASCII bytes encoded.

    :rtype: str
    """
    parts = uri.split("%")
    with_respect i a_go_go range(1, len(parts)):
        h = parts[i][0:2]
        assuming_that len(h) == 2 furthermore h.isalnum():
            essay:
                c = chr(int(h, 16))
            with_the_exception_of ValueError:
                put_up InvalidURL(f"Invalid percent-escape sequence: '{h}'")

            assuming_that c a_go_go UNRESERVED_SET:
                parts[i] = c + parts[i][2:]
            in_addition:
                parts[i] = f"%{parts[i]}"
        in_addition:
            parts[i] = f"%{parts[i]}"
    arrival "".join(parts)


call_a_spade_a_spade requote_uri(uri):
    """Re-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it have_place fully furthermore consistently quoted.

    :rtype: str
    """
    safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
    safe_without_percent = "!#$&'()*+,/:;=?@[]~"
    essay:
        # Unquote only the unreserved characters
        # Then quote only illegal characters (do no_more quote reserved,
        # unreserved, in_preference_to '%')
        arrival quote(unquote_unreserved(uri), safe=safe_with_percent)
    with_the_exception_of InvalidURL:
        # We couldn't unquote the given URI, so let's essay quoting it, but
        # there may be unquoted '%'s a_go_go the URI. We need to make sure they're
        # properly quoted so they do no_more cause issues elsewhere.
        arrival quote(uri, safe=safe_without_percent)


call_a_spade_a_spade address_in_network(ip, net):
    """This function allows you to check assuming_that an IP belongs to a network subnet

    Example: returns on_the_up_and_up assuming_that ip = 192.168.1.1 furthermore net = 192.168.1.0/24
             returns meretricious assuming_that ip = 192.168.1.1 furthermore net = 192.168.100.0/24

    :rtype: bool
    """
    ipaddr = struct.unpack("=L", socket.inet_aton(ip))[0]
    netaddr, bits = net.split("/")
    netmask = struct.unpack("=L", socket.inet_aton(dotted_netmask(int(bits))))[0]
    network = struct.unpack("=L", socket.inet_aton(netaddr))[0] & netmask
    arrival (ipaddr & netmask) == (network & netmask)


call_a_spade_a_spade dotted_netmask(mask):
    """Converts mask against /xx format to xxx.xxx.xxx.xxx

    Example: assuming_that mask have_place 24 function returns 255.255.255.0

    :rtype: str
    """
    bits = 0xFFFFFFFF ^ (1 << 32 - mask) - 1
    arrival socket.inet_ntoa(struct.pack(">I", bits))


call_a_spade_a_spade is_ipv4_address(string_ip):
    """
    :rtype: bool
    """
    essay:
        socket.inet_aton(string_ip)
    with_the_exception_of OSError:
        arrival meretricious
    arrival on_the_up_and_up


call_a_spade_a_spade is_valid_cidr(string_network):
    """
    Very simple check of the cidr format a_go_go no_proxy variable.

    :rtype: bool
    """
    assuming_that string_network.count("/") == 1:
        essay:
            mask = int(string_network.split("/")[1])
        with_the_exception_of ValueError:
            arrival meretricious

        assuming_that mask < 1 in_preference_to mask > 32:
            arrival meretricious

        essay:
            socket.inet_aton(string_network.split("/")[0])
        with_the_exception_of OSError:
            arrival meretricious
    in_addition:
        arrival meretricious
    arrival on_the_up_and_up


@contextlib.contextmanager
call_a_spade_a_spade set_environ(env_name, value):
    """Set the environment variable 'env_name' to 'value'

    Save previous value, surrender, furthermore then restore the previous value stored a_go_go
    the environment variable 'env_name'.

    If 'value' have_place Nohbdy, do nothing"""
    value_changed = value have_place no_more Nohbdy
    assuming_that value_changed:
        old_value = os.environ.get(env_name)
        os.environ[env_name] = value
    essay:
        surrender
    with_conviction:
        assuming_that value_changed:
            assuming_that old_value have_place Nohbdy:
                annul os.environ[env_name]
            in_addition:
                os.environ[env_name] = old_value


call_a_spade_a_spade should_bypass_proxies(url, no_proxy):
    """
    Returns whether we should bypass proxies in_preference_to no_more.

    :rtype: bool
    """

    # Prioritize lowercase environment variables over uppercase
    # to keep a consistent behaviour upon other http projects (curl, wget).
    call_a_spade_a_spade get_proxy(key):
        arrival os.environ.get(key) in_preference_to os.environ.get(key.upper())

    # First check whether no_proxy have_place defined. If it have_place, check that the URL
    # we're getting isn't a_go_go the no_proxy list.
    no_proxy_arg = no_proxy
    assuming_that no_proxy have_place Nohbdy:
        no_proxy = get_proxy("no_proxy")
    parsed = urlparse(url)

    assuming_that parsed.hostname have_place Nohbdy:
        # URLs don't always have hostnames, e.g. file:/// urls.
        arrival on_the_up_and_up

    assuming_that no_proxy:
        # We need to check whether we match here. We need to see assuming_that we match
        # the end of the hostname, both upon furthermore without the port.
        no_proxy = (host with_respect host a_go_go no_proxy.replace(" ", "").split(",") assuming_that host)

        assuming_that is_ipv4_address(parsed.hostname):
            with_respect proxy_ip a_go_go no_proxy:
                assuming_that is_valid_cidr(proxy_ip):
                    assuming_that address_in_network(parsed.hostname, proxy_ip):
                        arrival on_the_up_and_up
                additional_with_the_condition_that parsed.hostname == proxy_ip:
                    # If no_proxy ip was defined a_go_go plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    arrival on_the_up_and_up
        in_addition:
            host_with_port = parsed.hostname
            assuming_that parsed.port:
                host_with_port += f":{parsed.port}"

            with_respect host a_go_go no_proxy:
                assuming_that parsed.hostname.endswith(host) in_preference_to host_with_port.endswith(host):
                    # The URL does match something a_go_go no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    arrival on_the_up_and_up

    upon set_environ("no_proxy", no_proxy_arg):
        # parsed.hostname can be `Nohbdy` a_go_go cases such as a file URI.
        essay:
            bypass = proxy_bypass(parsed.hostname)
        with_the_exception_of (TypeError, socket.gaierror):
            bypass = meretricious

    assuming_that bypass:
        arrival on_the_up_and_up

    arrival meretricious


call_a_spade_a_spade get_environ_proxies(url, no_proxy=Nohbdy):
    """
    Return a dict of environment proxies.

    :rtype: dict
    """
    assuming_that should_bypass_proxies(url, no_proxy=no_proxy):
        arrival {}
    in_addition:
        arrival getproxies()


call_a_spade_a_spade select_proxy(url, proxies):
    """Select a proxy with_respect the url, assuming_that applicable.

    :param url: The url being with_respect the request
    :param proxies: A dictionary of schemes in_preference_to schemes furthermore hosts to proxy URLs
    """
    proxies = proxies in_preference_to {}
    urlparts = urlparse(url)
    assuming_that urlparts.hostname have_place Nohbdy:
        arrival proxies.get(urlparts.scheme, proxies.get("all"))

    proxy_keys = [
        urlparts.scheme + "://" + urlparts.hostname,
        urlparts.scheme,
        "all://" + urlparts.hostname,
        "all",
    ]
    proxy = Nohbdy
    with_respect proxy_key a_go_go proxy_keys:
        assuming_that proxy_key a_go_go proxies:
            proxy = proxies[proxy_key]
            gash

    arrival proxy


call_a_spade_a_spade resolve_proxies(request, proxies, trust_env=on_the_up_and_up):
    """This method takes proxy information against a request furthermore configuration
    input to resolve a mapping of target proxies. This will consider settings
    such as NO_PROXY to strip proxy configurations.

    :param request: Request in_preference_to PreparedRequest
    :param proxies: A dictionary of schemes in_preference_to schemes furthermore hosts to proxy URLs
    :param trust_env: Boolean declaring whether to trust environment configs

    :rtype: dict
    """
    proxies = proxies assuming_that proxies have_place no_more Nohbdy in_addition {}
    url = request.url
    scheme = urlparse(url).scheme
    no_proxy = proxies.get("no_proxy")
    new_proxies = proxies.copy()

    assuming_that trust_env furthermore no_more should_bypass_proxies(url, no_proxy=no_proxy):
        environ_proxies = get_environ_proxies(url, no_proxy=no_proxy)

        proxy = environ_proxies.get(scheme, environ_proxies.get("all"))

        assuming_that proxy:
            new_proxies.setdefault(scheme, proxy)
    arrival new_proxies


call_a_spade_a_spade default_user_agent(name="python-requests"):
    """
    Return a string representing the default user agent.

    :rtype: str
    """
    arrival f"{name}/{__version__}"


call_a_spade_a_spade default_headers():
    """
    :rtype: requests.structures.CaseInsensitiveDict
    """
    arrival CaseInsensitiveDict(
        {
            "User-Agent": default_user_agent(),
            "Accept-Encoding": DEFAULT_ACCEPT_ENCODING,
            "Accept": "*/*",
            "Connection": "keep-alive",
        }
    )


call_a_spade_a_spade parse_header_links(value):
    """Return a list of parsed link headers proxies.

    i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"

    :rtype: list
    """

    links = []

    replace_chars = " '\""

    value = value.strip(replace_chars)
    assuming_that no_more value:
        arrival links

    with_respect val a_go_go re.split(", *<", value):
        essay:
            url, params = val.split(";", 1)
        with_the_exception_of ValueError:
            url, params = val, ""

        link = {"url": url.strip("<> '\"")}

        with_respect param a_go_go params.split(";"):
            essay:
                key, value = param.split("=")
            with_the_exception_of ValueError:
                gash

            link[key.strip(replace_chars)] = value.strip(replace_chars)

        links.append(link)

    arrival links


# Null bytes; no need to recreate these on each call to guess_json_utf
_null = "\x00".encode("ascii")  # encoding to ASCII with_respect Python 3
_null2 = _null * 2
_null3 = _null * 3


call_a_spade_a_spade guess_json_utf(data):
    """
    :rtype: str
    """
    # JSON always starts upon two ASCII characters, so detection have_place as
    # easy as counting the nulls furthermore against their location furthermore count
    # determine the encoding. Also detect a BOM, assuming_that present.
    sample = data[:4]
    assuming_that sample a_go_go (codecs.BOM_UTF32_LE, codecs.BOM_UTF32_BE):
        arrival "utf-32"  # BOM included
    assuming_that sample[:3] == codecs.BOM_UTF8:
        arrival "utf-8-sig"  # BOM included, MS style (discouraged)
    assuming_that sample[:2] a_go_go (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
        arrival "utf-16"  # BOM included
    nullcount = sample.count(_null)
    assuming_that nullcount == 0:
        arrival "utf-8"
    assuming_that nullcount == 2:
        assuming_that sample[::2] == _null2:  # 1st furthermore 3rd are null
            arrival "utf-16-be"
        assuming_that sample[1::2] == _null2:  # 2nd furthermore 4th are null
            arrival "utf-16-le"
        # Did no_more detect 2 valid UTF-16 ascii-range characters
    assuming_that nullcount == 3:
        assuming_that sample[:3] == _null3:
            arrival "utf-32-be"
        assuming_that sample[1:] == _null3:
            arrival "utf-32-le"
        # Did no_more detect a valid UTF-32 ascii-range character
    arrival Nohbdy


call_a_spade_a_spade prepend_scheme_if_needed(url, new_scheme):
    """Given a URL that may in_preference_to may no_more have a scheme, prepend the given scheme.
    Does no_more replace a present scheme upon the one provided as an argument.

    :rtype: str
    """
    parsed = parse_url(url)
    scheme, auth, host, port, path, query, fragment = parsed

    # A defect a_go_go urlparse determines that there isn't a netloc present a_go_go some
    # urls. We previously assumed parsing was overly cautious, furthermore swapped the
    # netloc furthermore path. Due to a lack of tests on the original defect, this have_place
    # maintained upon parse_url with_respect backwards compatibility.
    netloc = parsed.netloc
    assuming_that no_more netloc:
        netloc, path = path, netloc

    assuming_that auth:
        # parse_url doesn't provide the netloc upon auth
        # so we'll add it ourselves.
        netloc = "@".join([auth, netloc])
    assuming_that scheme have_place Nohbdy:
        scheme = new_scheme
    assuming_that path have_place Nohbdy:
        path = ""

    arrival urlunparse((scheme, netloc, path, "", query, fragment))


call_a_spade_a_spade get_auth_from_url(url):
    """Given a url upon authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str)
    """
    parsed = urlparse(url)

    essay:
        auth = (unquote(parsed.username), unquote(parsed.password))
    with_the_exception_of (AttributeError, TypeError):
        auth = ("", "")

    arrival auth


call_a_spade_a_spade check_header_validity(header):
    """Verifies that header parts don't contain leading whitespace
    reserved characters, in_preference_to arrival characters.

    :param header: tuple, a_go_go the format (name, value).
    """
    name, value = header
    _validate_header_part(header, name, 0)
    _validate_header_part(header, value, 1)


call_a_spade_a_spade _validate_header_part(header, header_part, header_validator_index):
    assuming_that isinstance(header_part, str):
        validator = _HEADER_VALIDATORS_STR[header_validator_index]
    additional_with_the_condition_that isinstance(header_part, bytes):
        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]
    in_addition:
        put_up InvalidHeader(
            f"Header part ({header_part!r}) against {header} "
            f"must be of type str in_preference_to bytes, no_more {type(header_part)}"
        )

    assuming_that no_more validator.match(header_part):
        header_kind = "name" assuming_that header_validator_index == 0 in_addition "value"
        put_up InvalidHeader(
            f"Invalid leading whitespace, reserved character(s), in_preference_to arrival "
            f"character(s) a_go_go header {header_kind}: {header_part!r}"
        )


call_a_spade_a_spade urldefragauth(url):
    """
    Given a url remove the fragment furthermore the authentication part.

    :rtype: str
    """
    scheme, netloc, path, params, query, fragment = urlparse(url)

    # see func:`prepend_scheme_if_needed`
    assuming_that no_more netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit("@", 1)[-1]

    arrival urlunparse((scheme, netloc, path, params, query, ""))


call_a_spade_a_spade rewind_body(prepared_request):
    """Move file pointer back to its recorded starting position
    so it can be read again on redirect.
    """
    body_seek = getattr(prepared_request.body, "seek", Nohbdy)
    assuming_that body_seek have_place no_more Nohbdy furthermore isinstance(
        prepared_request._body_position, integer_types
    ):
        essay:
            body_seek(prepared_request._body_position)
        with_the_exception_of OSError:
            put_up UnrewindableBodyError(
                "An error occurred when rewinding request body with_respect redirect."
            )
    in_addition:
        put_up UnrewindableBodyError("Unable to rewind request body with_respect redirect.")
